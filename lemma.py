# -*- coding: utf-8 -*-
"""Lemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCzyZvd13TU5zDasdRmDmr4joDWNnAWY

Downloading and setting up the Stanza model for Afrikaans
"""

!pip install stanza

import stanza  # Import the stanza library

# Download and initialize the NLP pipeline for Afrikaans,
# which performs sentence splitting, tokenization, POS tagging, and lemmatization
stanza.download('af')  # Download the Afrikaans language model

# Create a text processing pipeline with specified processors
nlp = stanza.Pipeline('af', processors='tokenize,pos,lemma')

"""Text preprocessing function (lemmatization, replacement of proper nouns, pronouns, and numerals with tokens)"""

import re
from collections import OrderedDict
from typing import Callable

def prepare_afrikaans_text_stanza(
    input_path: str,
    output_path: str,
    nlp: Callable,
    replace_pron: bool = True,
    replace_propn: bool = False,
    replace_num: bool = True,
    merge_compound_nouns: bool = False,
    handle_separable_verbs: bool = False,
) -> None:
    """
    Processes Afrikaans text using Stanza NLP with optional normalization.

    Args:
        input_path (str): Path to input text file.
        output_path (str): Path to write processed text.
        nlp (Callable): Initialized Stanza pipeline for Afrikaans.
        replace_pron (bool): Replace personal pronouns with [PRON].
        replace_propn (bool): Replace proper nouns with [PROPN].
        replace_num (bool): Replace numerals with [NUM].
        merge_compound_nouns (bool): Naively split long compound nouns.
        handle_separable_verbs (bool): Naively separate verb prefixes.
    """

    ARTICLES = {"die", "'n", "al", "hierdie", "daardie", "sommige", "enigste"}
    PERSONAL_PRONOUNS = {"ek", "jy", "hy", "sy", "dit", "ons", "julle", "hulle", "u"}
    COMMON_VERB_PREFIXES = {"op", "af", "uit", "aan", "mee", "in"}

    with open(input_path, "r", encoding="utf-8") as fin, open(output_path, "w", encoding="utf-8") as fout:
        for line in fin:
            line = line.strip()
            if not line:
                continue

            doc = nlp(line)
            for sent in doc.sentences:
                tokens = []

                for word in sent.words:
                    text = word.text.lower()
                    lemma = word.lemma.lower() if word.lemma else text
                    upos = word.upos

                    if upos == "PUNCT":
                        continue

                    if upos == "DET" and text in ARTICLES:
                        tokens.append(text)

                    elif upos == "PRON":
                        if replace_pron and text in PERSONAL_PRONOUNS:
                            tokens.append("[PRON]")
                        else:
                            tokens.append(text)

                    elif upos == "PROPN":
                        tokens.append("[PROPN]" if replace_propn else text)

                    elif upos == "NUM":
                        tokens.append("[NUM]" if replace_num else text)

                    elif upos == "VERB" and handle_separable_verbs:
                        for prefix in COMMON_VERB_PREFIXES:
                            if lemma.startswith(prefix) and len(lemma) > len(prefix):
                                tokens.extend([lemma[len(prefix):], prefix])
                                break
                        else:
                            tokens.append(lemma)

                    elif merge_compound_nouns and upos == "NOUN" and "_" not in lemma:
                        if len(lemma) > 6:
                            tokens.extend([lemma[:3], lemma[3:]])  # Naive split
                        else:
                            tokens.append(lemma)

                    else:
                        tokens.append(lemma)

                if tokens:
                    fout.write(" ".join(tokens) + "\n")

"""Running preprocessing on the corpus"""

prepare_afrikaans_text_stanza('input_afrikaans.txt', 'output_afrikaans.txt', nlp)