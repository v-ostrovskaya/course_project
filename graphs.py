# -*- coding: utf-8 -*-
"""graphs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HAg3DaKmb1Uzr6kNHmnuQ749qD6E7Y93

Обучение
"""

!pip install --upgrade numpy gensim

from gensim.models import Word2Vec

# Load the text file where each line is a separate sentence with space-separated words
with open("output_afrikaans.txt", encoding="utf-8") as f:
    sentences = [line.strip().split() for line in f if line.strip()]

# Train the Word2Vec model (sg=1 for skip-gram)
model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4, sg=1)

# Example: find words similar to '[PRON]'
print(model.wv.most_similar('[PRON]', topn=5))

!pip install scikit-learn

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Select 100 most frequent words
words = list(model.wv.index_to_key[:100])
word_vectors = [model.wv[word] for word in words]

# Reduce dimensionality to 2D for visualization
pca = PCA(n_components=2)
result = pca.fit_transform(word_vectors)

# Clustering: select number of clusters (e.g., 5)
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
labels = kmeans.fit_predict(result)

# Color palette for clusters
colors = plt.cm.tab10(labels)

# Visualization
plt.figure(figsize=(14, 10))
plt.scatter(result[:, 0], result[:, 1], c=colors, s=50)

# Labels
for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=9)

plt.title("PCA visualization of word vectors with KMeans clustering")
plt.grid(True)
plt.show()

import numpy as np

# Parameters
m = model.vector_size  # Embedding dimension (100, 300, etc.)
n = 3  # n-gram size

# Open corpus where each line is a text split into words
with open('output_afrikaans.txt', encoding='utf-8') as f:
    lines = f.readlines()

output_vectors = []

for line in lines:
    words = line.strip().split()

    # Skip lines with fewer than n words
    if len(words) < n:
        continue

    # Process n-grams
    for i in range(len(words) - n + 1):
        ngram = words[i:i+n]

        # Get vector for each word
        vectors = []
        for w in ngram:
            if w in model.wv:
                vectors.append(model.wv[w])
            else:
                # If word not in vocabulary, use zero vector or skip
                vectors.append(np.zeros(m))

        # Concatenate into single vector
        ngram_vector = np.concatenate(vectors)

        # Add to results
        output_vectors.append(ngram_vector)

# Save to file (e.g., in numpy format)
np.save('ngram_vectors.npy', np.array(output_vectors))

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from collections import defaultdict
import matplotlib.patches as mpatches

# === 1. Sanity check ===
if len(word_vectors) != len(labels):
    raise ValueError("Length of word_vectors and labels must match.")

# === 2. Normalize vectors ===
scaler = StandardScaler()
vectors_scaled = scaler.fit_transform(word_vectors)

# === 3. Calculate centroids for each cluster ===
n_clusters = len(set(labels))
centroids = np.zeros((n_clusters, vectors_scaled.shape[1]))
counts = np.zeros(n_clusters)

for i, label in enumerate(labels):
    centroids[label] += vectors_scaled[i]
    counts[label] += 1

counts[counts == 0] = 1e-9  # avoid division by zero
centroids /= counts[:, None]

# === 4. Build graph and collect top words ===
G = nx.Graph()
cluster_words = defaultdict(list)

for word, vec, label in zip(words, vectors_scaled, labels):
    cluster_words[label].append((word, vec))

cluster_labels = {}
for cluster_id in range(n_clusters):
    if cluster_id not in cluster_words:
        continue
    word_vecs = cluster_words[cluster_id]
    centroid = centroids[cluster_id]
    word_vecs.sort(key=lambda x: np.linalg.norm(x[1] - centroid))
    top_words = [wv[0] for wv in word_vecs[:5]]
    label = ", ".join(top_words)
    cluster_labels[cluster_id] = label
    G.add_node(cluster_id, label=label)

existing_cluster_ids = list(G.nodes())
filtered_centroids = centroids[existing_cluster_ids]

# === 5. Add edges based on pairwise distance threshold ===
if len(filtered_centroids) > 1:
    distances = euclidean_distances(filtered_centroids)
    threshold = np.percentile(distances, 70)

    for i_idx in range(len(filtered_centroids)):
        for j_idx in range(i_idx + 1, len(filtered_centroids)):
            dist = distances[i_idx, j_idx]
            if dist < threshold:
                cid_i = existing_cluster_ids[i_idx]
                cid_j = existing_cluster_ids[j_idx]
                G.add_edge(cid_i, cid_j, weight=round(dist, 2))

# === 6. Visualize the cluster graph ===
if G.nodes():
    plt.figure(figsize=(14, 10))

    # Project centroids to 2D using PCA
    pca = PCA(n_components=2)
    centroids_2d = pca.fit_transform(filtered_centroids)
    scale = 2.5
    centroids_2d_scaled = centroids_2d / np.max(np.abs(centroids_2d)) * scale
    pos = {cid: centroids_2d_scaled[i] for i, cid in enumerate(existing_cluster_ids)}

    # Node size proportional to number of words
    node_sizes = [max(len(cluster_words[cid]) * 300, 500) for cid in existing_cluster_ids]

    # Draw nodes
    nx.draw_networkx_nodes(
        G, pos,
        node_color='lightblue',
        node_size=node_sizes,
        alpha=0.9,
        edgecolors='black'
    )

    # Draw edges with inverse width to distance
    edges = G.edges(data=True)
    weights = [edge[2]['weight'] for edge in edges]
    max_weight = max(weights) if weights else 1
    widths = [5 * (1 - w / max_weight) + 1 for w in weights]

    nx.draw_networkx_edges(
        G, pos,
        width=widths,
        alpha=0.7,
        edge_color='gray'
    )

    # Draw labels with larger font
    nx.draw_networkx_labels(
        G, pos,
        labels=cluster_labels,
        font_size=12,
        font_weight='bold'
    )

    # Title and legend
    plt.title("Cluster Graph — Conceptual Knowledge Tree", fontsize=18)

    # Legend for node sizes
    sizes_for_legend = [500, 1000, 2000]
    patches = [
        mpatches.Circle((0, 0), radius=np.sqrt(s / np.pi), facecolor='lightblue', edgecolor='black')
        for s in sizes_for_legend
    ]
    labels_legend = [f"{int(s / 300)} words" for s in sizes_for_legend]
    plt.legend(patches, labels_legend, title="Node size = number of words", loc='upper right')

    plt.axis('off')
    plt.tight_layout()
    plt.show()
else:
    print("No nodes to visualize.")